Witajcie w ósmym i ostatnim odcinku pierwszej serii podcastu "Marek AI". Przeszliśmy długą drogę, od definicji sztucznej inteligencji, przez uczenie maszynowe, wielkie modele językowe, aż po autonomiczne zespoły agentów. Dzisiaj zajmiemy się tematem, który przewijał się w naszych rozmowach wielokrotnie i który jest jednym z największych wyzwań współczesnej AI – zjawiskiem halucynacji.

Czym dokładnie są halucynacje AI? Wbrew nazwie, która może kojarzyć się z ludzkimi doznaniami, halucynacje w kontekście modeli językowych to po prostu generowanie informacji, które są nieprawdziwe, sprzeczne z faktami lub całkowicie wymyślone, ale przedstawione w sposób, który brzmi bardzo wiarygodnie i autorytatywnie. Model może zmyślić cytat, podać nieprawidłową datę historyczną, opisać nieistniejące badanie naukowe lub wymyślić biografię osoby.

Dlaczego tak się dzieje? Aby to zrozumieć, musimy przypomnieć sobie, jak działają LLM-y, o czym mówiliśmy w trzecim odcinku. W swojej istocie, są to maszyny do statystycznego przewidywania następnego słowa. Nie mają one prawdziwego modelu rzeczywistości ani świadomości prawdy i fałszu. Ich celem jest stworzenie gramatycznie poprawnego i spójnego tekstu, który pasuje do wzorców, jakich nauczyły się z danych treningowych.

Czasami, aby zachować tę spójność i "płynność" odpowiedzi, model "wypełnia luki" w swojej wiedzy, generując najbardziej prawdopodobną statystycznie sekwencję słów, nawet jeśli nie ma ona pokrycia w faktach. To właśnie jest mechanizm powstawania halucynacji. To nie jest "kłamstwo" w ludzkim rozumieniu, bo model nie ma intencji oszukiwania. To po prostu artefakt jego probabilistycznej natury.

Halucynacje są poważnym problemem, ponieważ podważają zaufanie do technologii AI. Jeśli nie możemy być pewni, czy informacja wygenerowana przez model jest prawdziwa, jej użyteczność w wielu zastosowaniach, takich jak medycyna, prawo czy badania naukowe, drastycznie spada.

Jak więc możemy sobie radzić z tym problemem? Istnieje kilka technik i strategii minimalizowania ryzyka halucynacji.

Jedną z najważniejszych jest "uziemienie" (grounding). Polega ono na dostarczeniu modelowi konkretnego, wiarygodnego źródła danych, na którym ma oprzeć swoją odpowiedź, zamiast polegać wyłącznie na swojej wewnętrznej, ogólnej wiedzy. Najpopularniejszą techniką uziemienia jest RAG, czyli Retrieval-Augmented Generation (Generowanie Wzbogacone o Wyszukiwanie).

Jak działa RAG? Kiedy zadajemy pytanie, system najpierw nie pyta o nie modelu językowego. Zamiast tego, przeszukuje naszą bazę wiedzy – na przykład zbiór dokumentów firmowych, artykułów naukowych czy transkrypcji podcastów – w poszukiwaniu fragmentów najbardziej relevantnych dla naszego zapytania. Dopiero wtedy, te znalezione fragmenty są dołączane do naszego oryginalnego pytania jako dodatkowy kontekst i całość jest wysyłana do LLM-a z instrukcją: "Odpowiedz na to pytanie, opierając się wyłącznie na dostarczonym kontekście." To zmusza model do trzymania się faktów zawartych w źródle i znacząco redukuje ryzyko wymyślania informacji. To właśnie RAG jest sercem chatbota, z którym możecie porozmawiać na stronie tego podcastu.

Inne techniki to na przykład weryfikacja i cytowanie. Możemy zbudować system, w którym po wygenerowaniu odpowiedzi przez jeden model, inny model lub algorytm próbuje zweryfikować każdą informację, porównując ją z zewnętrznymi źródłami wiedzy, na przykład z wyszukiwarką internetową. Jeśli informacja nie może być potwierdzona, jest oznaczana jako potencjalnie niewiarygodna.

Co ciekawe, skłonność modeli do "wymyślania" nie zawsze jest wadą. W zadaniach kreatywnych, takich jak pisanie opowiadań, tworzenie scenariuszy czy burze mózgów, ta zdolność do generowania nowych, nieoczywistych połączeń może być niezwykle cenna. To, co w jednym kontekście jest niebezpieczną halucynacją, w innym staje się pożądaną "kreatywnością". Kluczem jest zrozumienie natury modelu i świadome wykorzystywanie jego cech w zależności od zadania, które chcemy zrealizować.

Podsumowując, halucynacje AI to generowanie wiarygodnie brzmiących, ale nieprawdziwych informacji, wynikające ze statystycznej natury modeli językowych. Stanowią one poważne wyzwanie dla wiarygodności AI. Możemy je minimalizować za pomocą technik takich jak uziemienie, w szczególności RAG, oraz poprzez mechanizmy weryfikacji. Jednocześnie, musimy pamiętać, że ta sama cecha, która prowadzi do halucynacji, w kontekstach kreatywnych może być źródłem inspiracji.

Tym odcinkiem kończymy pierwszą serię podcastu "Marek AI", poświęconą fundamentom sztucznej inteligencji. Mam nadzieję, że te osiem odcinków dało Wam solidne podstawy i rozbudziło Waszą ciekawość. Dziękuję Wam za wspólną podróż po tym fascynującym świecie. Do usłyszenia w przyszłych projektach!
